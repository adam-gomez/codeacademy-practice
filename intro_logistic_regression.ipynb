{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression is a supervised machine learning algorithm that uses regression to predict the continuous probability, ranging from 0 to 1, of a data sample belonging to a specific category, or class. \n",
    "- Then, based on that probability, the sample is classified as belonging to the more probable class, ultimately making Logistic Regression a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours_studied</th>\n",
       "      <th>exam_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hours_studied  exam_results\n",
       "0              0             0\n",
       "1              1             0\n",
       "2              2             0\n",
       "3              3             0\n",
       "4              4             0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose we have 20 students and they formed the following dataset\n",
    "d = {'hours_studied': list(range(20)), 'exam_results': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head()\n",
    "# hours_studied represents the number of hours they studied for an exam\n",
    "# exam_results represents their failure (0) or success (1) on the exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets make a scatter plot of the data\n",
    "plt.scatter(df.hours_studied, df.exam_results, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a visual inspection, it looks like students who studied 10 or more hours\n",
    "# Were more likely to pass the exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Approach\n",
    "\n",
    "Recall that in Linear Regression, we fit a regression line of the following form to the data:\n",
    "\n",
    "    y = b0 + b1x1 + b2x2 + ... + bnxn\n",
    "\n",
    "- Where y is the value we are trying to predict\n",
    "- b0 is the intercept of the regression line\n",
    "- b1, b2, ... bn are the coefficients of the features x1, x2, ... xn of the regression line\n",
    "\n",
    "For our data points y is either 1 (success) or 0 (failing) and we have one feature (hours_studied). If we fit a Linear Regression model to our data and plotted the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(df.hours_studied.values.reshape(-1,1), df.exam_results.values.reshape(-1,1))\n",
    "y_predict = regr.predict(df.hours_studied.values.reshape(-1,1))\n",
    "plt.plot(df.hours_studied.values, y_predict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that at 0 hours studied, we have a negative probability predicted, and at higher numbers of hours studied we have probabilities greater than 1.0 predicted.\n",
    "\n",
    "These probabilities greater than 1.0 and less than 0.0 are meaningless!\n",
    "\n",
    "We get these meaningless probabilities because the output of a Linear Regression model ranges from negative infinity to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Defined\n",
    "\n",
    "To predict the probability of a data sample belonging to a class, we:\n",
    "\n",
    "1. Initialize all feature coefficients and intercept to 0\n",
    "2. Multiply each of the feature coefficients by their respective feature value to get what is known as the log-odds\n",
    "3. place the log-odds into the sigmoid function to link the output to the range `[0,1]`, giving us a probability\n",
    "\n",
    "By comparing the predicted probabilities to the actual classes of our data points, we can evaluate how well our model makes predictions and use gradient descent to update the coefficients and find the best ones for our model.\n",
    "\n",
    "To then make a final classification, we use a classification threshold to determine whether the data sample belongs to the positive class or the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-odds are another way of expressing the probability of a sample belonging to the positive class, or a student passing the exam. \n",
    "\n",
    "In probability, we calculate the odds of an event occurring as follows:\n",
    "\n",
    "Odds = P(event occurring) / P(event not occurring)\n",
    "\n",
    "The odds tell us how many more times likely an event is to occur than not occur. If a student will pass the exam with probability 0.7, they will fail with probability 1 - 0.7 = 0.3.\n",
    "\n",
    "We can then calculate the odds of passing as: \n",
    "\n",
    "    Odds of passing = 0.7/0.3 = 2.33\n",
    "    At this P(pass), a student is 2.33 times more likely to pass than to fail. \n",
    "    \n",
    "The log-odds are then understood as the logarithm of the odds:\n",
    "\n",
    "    Log odds of passing = log(2.33) = 0.847\n",
    "    \n",
    "For our Logistic Regression model, however, we calculate the log-odds, represented by z below, by summing the product of each feature value by its respective coefficient and adding the intercept. This allows us to map our feature values to a measure of how likely it is that a data sample belongs to the positive class.\n",
    "\n",
    "    z = b0 + b1x1 + ... + bnxn\n",
    "\n",
    "- b0 is the intercept\n",
    "- b1, b2, ... bn are the coefficients of the features x1, x2, ... xn\n",
    "\n",
    "In our previous example, we only had one feature value (x1), so we would only have one coefficient. But if we also had captured an additional feature (like number of cups of coffee), we would have a coefficient for that feature. These coefficients would likely be very different, as the relative impact of studying vs. drinking coffee would be quite different. \n",
    "\n",
    "This kind of multiplication and summing is known as a dot product.\n",
    "\n",
    "We can perform a dot product using numpy‘s np.dot() method! Given feature matrix features, coefficient vector coefficients, and an intercept, we can calculate the log-odds in numpy as follows:\n",
    "\n",
    "    log_odds = np.dot(features, coefficients) + intercept\n",
    "    \n",
    "np.dot() will take each row, or student, in features and multiply each individual feature value by its respective coefficient in coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s create a function log_odds that takes features, coefficients and intercept as parameters.\n",
    "def log_odds(features, coefficients, intercept):\n",
    "    return np.dot(features, coefficients) + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_coefficients = [[0.20678491]]\n",
    "intercept = [-1.76125712]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.76125712]\n",
      " [-1.55447221]\n",
      " [-1.3476873 ]\n",
      " [-1.14090239]\n",
      " [-0.93411748]\n",
      " [-0.72733257]\n",
      " [-0.52054766]\n",
      " [-0.31376275]\n",
      " [-0.10697784]\n",
      " [ 0.09980707]\n",
      " [ 0.30659198]\n",
      " [ 0.51337689]\n",
      " [ 0.7201618 ]\n",
      " [ 0.92694671]\n",
      " [ 1.13373162]\n",
      " [ 1.34051653]\n",
      " [ 1.54730144]\n",
      " [ 1.75408635]\n",
      " [ 1.96087126]\n",
      " [ 2.16765617]]\n"
     ]
    }
   ],
   "source": [
    "calculated_log_odds = log_odds(df.hours_studied.values.reshape(-1,1), calculated_coefficients, intercept)\n",
    "print(calculated_log_odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Function\n",
    "\n",
    "The Sigmoid Function is a special case of the more general Logistic Function, where Logistic Regression gets its name. \n",
    "\n",
    "Why is the Sigmoid Function so important? \n",
    "\n",
    "By plugging the log-odds into the Sigmoid Function, defined below, we map the log-odds `z` to the range `[0,1]`\n",
    "\n",
    "    h(z) = 1 / (1 + e ** -z)\n",
    "    \n",
    "- `e ** -z` is the exponential function, which van be written in `numpy` as `np.exp(-z)`\n",
    "\n",
    "This enables our Logistic Regression model to output the probability of a sample belonging to the positive class, or in our case, a student passing the final exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a sigmoid function of our own\n",
    "def sigmoid(z):\n",
    "    denominator = 1 + np.exp(-z)\n",
    "    return 1/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14663296]\n",
      " [0.17444128]\n",
      " [0.20624873]\n",
      " [0.24215472]\n",
      " [0.28209011]\n",
      " [0.32578035]\n",
      " [0.37272418]\n",
      " [0.42219656]\n",
      " [0.47328102]\n",
      " [0.52493108]\n",
      " [0.57605318]\n",
      " [0.62559776]\n",
      " [0.67264265]\n",
      " [0.71645543]\n",
      " [0.7565269 ]\n",
      " [0.79257487]\n",
      " [0.82452363]\n",
      " [0.85246747]\n",
      " [0.87662721]\n",
      " [0.89730719]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sigmoid of the log-odds\n",
    "probabilities = sigmoid(calculated_log_odds)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Loss\n",
    "\n",
    "Now that we understand how a Logistic Regression model makes its probability predictions, what coefficients and intercept should we use in our model to best predict whether a student will pass the exam?\n",
    "\n",
    "In the example above, we were supplied the coefficients and intercept. How can we calculate them?\n",
    "\n",
    "The function used to evaluate the performance of a machine learning model is called a loss function, or a cost function. To evaluate how “good a fit” a model is, we calculate the loss for each data sample (how wrong the model’s prediction was) and then average the loss across all samples.\n",
    "\n",
    "The loss function for Logistic Regression, known as Log Loss, is given below:\n",
    "\n",
    "    -1/m * sum(i=1 to m)[(y_i)log(h(z_i))+(1 - y_i)log(1 - h(z_i))]\n",
    "    \n",
    "- `m` is the total number of data samples\n",
    "- `y_i` is the class of data sample `i`\n",
    "- `z_i` is the log-odds of sample `i`\n",
    "- `h(z_i)` is the sigmoid of the log-odds of sample `i` which is the probability of sample `i` belonging to the positive class\n",
    "\n",
    "The goal of the Logistic Regression model is to the find the feature coefficients and intercept, which shape the logistic function, that minimize log-loss for our training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking Down Log-Loss Function\n",
    "Lets break down our log-loss function into two separate parts. \n",
    "\n",
    "#### Consider the case when a data sample has class `y = 1` (success, in the case of our data). \n",
    "\n",
    "The right side of the equation (after the +) drops out, because we would end up with (1 - y_i) turning into (1 - 1) = 0. That 0 then is multiplied by some value, but still ends up being 0. \n",
    "\n",
    "In this case (success), the loss for that individual student becomes:\n",
    "    \n",
    "    loss(where y=1) = -log(h(z_i))\n",
    "    \n",
    "In other words, the loss is just the log of the probability the student passed the exam. \n",
    "\n",
    "#### And for a student who failed the exam, the sample has class `y = 0`\n",
    "\n",
    "The left side of the equation would drop out (become equal to 0), and the loss for that individual student becomes:\n",
    "\n",
    "    loss(where y=0) = -log(1-h(z_i))\n",
    "    \n",
    "In other words, the loss is the log of one minus the probability of success. What is one minus the probability of success? The probability of failure!\n",
    "\n",
    "So lets reason through this. \n",
    "- In the case of a student who actually passed the exam, as our model's predicted probability for this student approaches 1 (100% chance of passing), the amount of loss calculated at this point approaches 0. Confident correct predictions result in small losses.\n",
    "- In the case of a student who actually failed the exam, as our model's predicted probability for this student approachs 1 (100% chance of passing), the amount of loss calculated at this point approaches infinity. Confident incorrect predictions result in large losses.\n",
    "\n",
    "Therefore, coefficients and intercepts that have higher probabilities for passing students and lower probabilities for failing students will have the overall least log loss.\n",
    "\n",
    "We can use gradient descent to find the coefficients that minimize log-loss across all of our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate log-loss\n",
    "def log_loss(probabilities,actual_class):\n",
    "    return np.sum(-(1/actual_class.shape[0])*(actual_class*np.log(probabilities) + (1-actual_class)*np.log(1-probabilities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.398640332141742\n"
     ]
    }
   ],
   "source": [
    "# Lets call this function using the probabilities we previously calculated (we got these by condensing the log odds into a sigmoid function)\n",
    "passed_exam = df.exam_results.values.reshape(-1,1) # For clarity, we will store the exam_results column of our DataFrame in a simple array called passed_exam\n",
    "loss = log_loss(probabilities, passed_exam) # We pass the probabilities (calculated using our provided coefficient and intercept) and the true classes (success or failure) for each observation\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11],\n",
       "       [12],\n",
       "       [13],\n",
       "       [14],\n",
       "       [15],\n",
       "       [16],\n",
       "       [17],\n",
       "       [18],\n",
       "       [19]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loss is pretty good! (That's because the coefficient and intercept were already excellent choices)\n",
    "# What if we had used a coefficient and intercept of 0,0?\n",
    "num_hours = df.hours_studied.values.reshape(-1,1) # For clarity, we will store the hours_studied column of our DataFrame in a simple array called num_hours\n",
    "num_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lousy_calculated_log_odds = log_odds(num_hours, coefficients = 0, intercept = 0)\n",
    "lousy_calculated_log_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sigmoid of the log-odds\n",
    "lousy_probabilities = sigmoid(lousy_calculated_log_odds)\n",
    "print(lousy_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# Lets plug in our results into our log_loss function\n",
    "lousy_loss = log_loss(lousy_probabilities, passed_exam)\n",
    "print(lousy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start with a coefficient of 0 and intercept of 0, then we end up with a larger loss value. \n",
    "\n",
    "We can start any determination of optimal coefficients and intercepts with 0,0 and use gradient descent to find the lowest loss in a stepwise manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Thresholding\n",
    "\n",
    "The default threshold for many algorithms is 0.5. \n",
    "\n",
    "If the predicted probability of an observation belonging to the positive class is greater than or equal to the threshold, 0.5, the classification of the sample is the positive class. \n",
    "\n",
    "If the predicted probability of an observation belonging to the positive class is less than the threshold, 0.5, the classification of the sample is the negative class.\n",
    "\n",
    "We can choose to change the threshold of classification based on the use-case of our model. For example, if we are creating a Logistic Regression model that classifies whether or not an individual has cancer, we want to be more sensitive to the positive cases, signifying the presence of cancer, than the negative cases. In this case, we could increase our sensitivity (recall) by lowering the threshold to 0.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function that performs thresholding and makes class predictions\n",
    "def predict_class(features, coefficients, intercept, threshold):\n",
    "    calculated_log_odds = log_odds(features, coefficients, intercept) # First we calculate log_odds\n",
    "    probabilities = sigmoid(calculated_log_odds) # Then we condense those log odds into a sigmoid\n",
    "    return np.where(probabilities >= threshold, 1,0) # Then we return an array of predicted successes (1) where the probability is greater than the threshold and failures (0) where the probability is less than the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Lets call predict class using our top-notch coefficients and intercept\n",
    "calculated_coefficients = [[0.20678491]]\n",
    "intercept = [-1.76125712]\n",
    "predicted_results = predict_class(num_hours, calculated_coefficients, intercept, threshold = 0.5)\n",
    "print(predicted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets overlay our original scatterplot with our predicted scatterplot\n",
    "plt.scatter(num_hours, passed_exam, color='black', alpha = 0.5, s = 200)\n",
    "plt.scatter(num_hours, predicted_results, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model was able to correctly predict 18 out of 20 students. There were 2 false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn\n",
    "\n",
    "To take advantage of sklearn‘s abilities, we can begin by creating a LogisticRegression object.\n",
    "\n",
    "`model = LogisticRegression()`\n",
    "\n",
    "After creating the object, we need to fit our model on the data. When we fit the model with sklearn it will perform gradient descent, repeatedly updating the coefficients of our model in order to minimize the log-loss.\n",
    "\n",
    "`model.fit(features, labels)`\n",
    "\n",
    "The first parameter is a matrix of features, and the second parameter is a matrix of class labels.\n",
    "\n",
    "Now that the model is trained, we can access a few useful attributes of the LogisticRegression object.\n",
    "\n",
    "- `model.coef_` is a vector of the coefficients of each feature\n",
    "- `model.intercept_` is the intercept `b0`\n",
    "\n",
    "With our trained model we are able to predict whether new data points belong to the positive class using the `.predict()` method.\n",
    "`.predict()` takes a matrix of features as a parameter and returns a vector of labels `1` or `0` for each sample. In making its predictions, sklearn uses a classification threshold of `0.5`.\n",
    "\n",
    "`model.predict(features)`\n",
    "\n",
    "If we are more interested in the predicted probability of the data samples belonging to the positive class than the actual class, we can use the `.predict_proba()` method. `predict_proba()` also takes a matrix of features as a parameter and returns a vector of probabilities, ranging from `0` to `1`, for each sample.\n",
    "\n",
    "`model.predict_proba(features)`\n",
    "\n",
    "#### Before proceeding, one important note is that sklearn‘s Logistic Regression implementation requires feature data to be normalized.\n",
    "\n",
    "Normalization scales all feature data to vary over the same range. sklearn‘s Logistic Regression requires normalized feature data due to a technique called Regularization that it uses under the hood. Regularization is out of the scope of this lesson, but in order to ensure the best results from our model, we will be using a normalized version of the data.\n",
    "\n",
    "#### SIDE LESSON ON NORMALIZATION\n",
    "#### Min-Max Normalization\n",
    "Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.\n",
    "\n",
    "Min-max normalization has one fairly significant downside: it does not handle outliers very well. For example, if you have 99 values between 0 and 40, and one value is 100, then the 99 values will all be transformed to a value between 0 and 0.4. \n",
    "\n",
    "#### Z-Score Normalization\n",
    "If a value is exactly equal to the mean of all the values of the feature, it will be normalized to 0. If it is below the mean, it will be a negative number, and if it is above the mean it will be a positive number. The size of those negative and positive numbers is determined by the standard deviation of the original feature.\n",
    "\n",
    "The only potential downside is that the features aren’t on the exact same scale. However, if outliers are present, this is the preferred method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.64750894],\n",
       "       [-1.47408695],\n",
       "       [-1.30066495],\n",
       "       [-1.12724296],\n",
       "       [-0.95382097],\n",
       "       [-0.78039897],\n",
       "       [-0.60697698],\n",
       "       [-0.43355498],\n",
       "       [-0.26013299],\n",
       "       [-0.086711  ],\n",
       "       [ 0.086711  ],\n",
       "       [ 0.26013299],\n",
       "       [ 0.43355498],\n",
       "       [ 0.60697698],\n",
       "       [ 0.78039897],\n",
       "       [ 0.95382097],\n",
       "       [ 1.12724296],\n",
       "       [ 1.30066495],\n",
       "       [ 1.47408695],\n",
       "       [ 1.64750894]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hours_studied_scaled = np.array([-1.64750894, -1.47408695, -1.30066495, -1.12724296, -0.95382097, -0.78039897, -0.60697698, -0.43355498, -0.26013299, -0.086711, 0.086711, 0.26013299, 0.43355498, 0.60697698, 0.78039897, 0.95382097, 1.12724296, 1.30066495, 1.47408695, 1.64750894])\n",
    "hours_studied_scaled = hours_studied_scaled.reshape(-1,1) \n",
    "hours_studied_scaled # Z-score normalized hours studied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(hours_studied_scaled, passed_exam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.72410099]] [-0.33110604]\n"
     ]
    }
   ],
   "source": [
    "# Save the model coefficients and intercept here\n",
    "calculated_coefficients, intercept = model.coef_, model.intercept_\n",
    "print(calculated_coefficients, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next semester a group of students in the Introductory Machine Learning course want to predict their final exam scores based on how much they intended to study for the exam. The number of hours each student thinks they will study, normalized, is given in guessed_hours_scaled. Use model to predict the probability that each student will pass the final exam, and save the probabilities to passed_predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30066495e+00],\n",
       "       [-1.30066495e+00],\n",
       "       [-9.53820966e-01],\n",
       "       [-7.80398973e-01],\n",
       "       [-7.80398973e-01],\n",
       "       [-7.80398973e-01],\n",
       "       [-6.06976979e-01],\n",
       "       [-2.60132991e-01],\n",
       "       [-2.60132991e-01],\n",
       "       [-8.67109970e-02],\n",
       "       [ 8.67109970e-02],\n",
       "       [ 8.67109970e-02],\n",
       "       [ 2.60132991e-01],\n",
       "       [ 6.06976979e-01],\n",
       "       [ 6.06976979e-01],\n",
       "       [ 9.53820966e-01],\n",
       "       [ 1.47408695e+00],\n",
       "       [ 1.64750894e+00],\n",
       "       [ 3.31669563e+02]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guessed_hours_scaled = np.array([-1.30066495e+00, -1.30066495e+00, -9.53820966e-01, -7.80398973e-01, -7.80398973e-01, -7.80398973e-01, -6.06976979e-01, -2.60132991e-01, -2.60132991e-01, -8.67109970e-02, 8.67109970e-02, 8.67109970e-02, 2.60132991e-01, 6.06976979e-01, 6.06976979e-01, 9.53820966e-01, 1.47408695e+00, 1.64750894e+00, 3.31669563e+02])\n",
    "guessed_hours_scaled = guessed_hours_scaled.reshape(-1,1)\n",
    "guessed_hours_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.929142   0.070858  ]\n",
      " [0.929142   0.070858  ]\n",
      " [0.87821024 0.12178976]\n",
      " [0.84245282 0.15754718]\n",
      " [0.84245282 0.15754718]\n",
      " [0.84245282 0.15754718]\n",
      " [0.79860457 0.20139543]\n",
      " [0.68559424 0.31440576]\n",
      " [0.68559424 0.31440576]\n",
      " [0.61789062 0.38210938]\n",
      " [0.54527751 0.45472249]\n",
      " [0.54527751 0.45472249]\n",
      " [0.47068628 0.52931372]\n",
      " [0.32841036 0.67158964]\n",
      " [0.32841036 0.67158964]\n",
      " [0.21192219 0.78807781]\n",
      " [0.09882323 0.90117677]\n",
      " [0.07520421 0.92479579]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "passed_predictions = model.predict_proba(guessed_hours_scaled)\n",
    "print(passed_predictions) # Probabilities are shown for (failure,success) for each student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That same semester, the Data Science department decides to update the final exam passage model to consider two features instead of just one. During the final exam, students were asked to estimate how much time they spent studying, as well as how many previous math courses they have taken. The student responses, along with their exam results, were split into training and test sets. The training features, normalized, are given to you in `exam_features_scaled_train`, and the students’ results on the final are given in `passed_exam_2_train`.\n",
    "\n",
    "Create a new Logistic Regression model named `model_2` and train it on `exam_features_scaled_train` and `passed_exam_2_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.26013299, -1.25537916],\n",
       "       [ 0.26013299, -0.49454331],\n",
       "       [ 1.64750894, -1.25537916],\n",
       "       [-1.12724296, -1.25537916],\n",
       "       [-0.43355498,  1.02712841],\n",
       "       [-0.60697698,  0.26629255],\n",
       "       [ 0.60697698,  1.02712841],\n",
       "       [-0.78039897, -0.49454331],\n",
       "       [ 0.086711  , -1.25537916],\n",
       "       [ 0.43355498,  0.26629255],\n",
       "       [ 0.95382097,  0.26629255],\n",
       "       [-1.47408695, -0.49454331],\n",
       "       [-1.64750894, -1.25537916],\n",
       "       [ 1.30066495,  1.02712841],\n",
       "       [-0.086711  ,  0.26629255]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam_features_scaled_train = np.array([-0.26013299, -1.25537916, 0.26013299, -0.49454331, 1.64750894, -1.25537916, -1.12724296, -1.25537916, -0.43355498, 1.02712841, -0.60697698, 0.26629255, 0.60697698, 1.02712841, -0.78039897, -0.49454331, 0.086711, -1.25537916, 0.43355498, 0.26629255, 0.95382097, 0.26629255, -1.47408695, -0.49454331, -1.64750894, -1.25537916, 1.30066495, 1.02712841, -0.086711, 0.26629255])\n",
    "exam_features_scaled_train = exam_features_scaled_train.reshape(-1,2)\n",
    "exam_features_scaled_train # Z-score normalized exam_features_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passed_exam_2_train = np.array([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1])\n",
    "passed_exam_2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = LogisticRegression()\n",
    "model_2.fit(exam_features_scaled_train, passed_exam_2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model you just trained to predict whether each student in the test set, `exam_features_scaled_test`, will pass the exam and save the predictions to `passed_predictions_2`. Print `passed_predictions_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.12724296, -0.49454331],\n",
       "       [ 0.78039897,  1.78796426],\n",
       "       [-0.95382097,  1.02712841],\n",
       "       [-1.3006649 ,  1.78796426],\n",
       "       [ 1.47408695, -0.49454331]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam_features_scaled_test = np.array([ 1.12724296, -0.49454331, 0.78039897, 1.78796426, -0.95382097, 1.02712841, -1.3006649, 1.78796426, 1.47408695, -0.49454331])\n",
    "exam_features_scaled_test = exam_features_scaled_test.reshape(-1, 2)\n",
    "exam_features_scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "passed_predictions_2 = model_2.predict(exam_features_scaled_test)\n",
    "print(passed_predictions_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predictions to the actual student performance on the exam in the test set. How well did your model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passed_exam_2_test = np.array([1, 1, 0, 1, 1])\n",
    "passed_exam_2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model_2 accuracy was .80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "One of the defining features of Logistic Regression is the interpretability we have from the feature coefficients. How to handle interpreting the coefficients depends on the kind of data you are working with (normalized or not) and the specific implementation of Logistic Regression you are using. We’ll discuss how to interpret the feature coefficients from a model created in sklearn with normalized feature data.\n",
    "\n",
    "Since our data is normalized, all features vary over the same range. Given this understanding, we can compare the feature coefficients’ magnitudes and signs to determine which features have the greatest impact on class prediction, and if that impact is positive or negative.\n",
    "\n",
    "- Features with larger, positive coefficients will increase the probability of a data sample belonging to the positive class\n",
    "- Features with larger, negative coefficients will decrease the probability of a data sample belonging to the positive class\n",
    "- Features with small, positive or negative coefficients have minimal impact on the probability of a data sample belonging to the positive class\n",
    "\n",
    "Given cancer data, a logistic regression model can let us know what features are most important for predicting survival after, for example, five years from diagnosis. Knowing these features can lead to a better understanding of outcomes, and even lives saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0536012197800966, 1.3782852632090778]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = model_2.coef_\n",
    "coefficients = coefficients.tolist()[0]\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar([1,2],coefficients)\n",
    "plt.xticks([1,2],['hours studied','math courses taken'])\n",
    "plt.xlabel('feature')\n",
    "plt.ylabel('coefficient')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "- Logistic Regression is used to perform binary classification, predicting whether a data sample belongs to a positive (present) class, labeled `1` and the negative (absent) class, labeled `0`.\n",
    "- The Sigmoid Function bounds the product of feature values and their coefficients, known as the log-odds, to the range `[0,1]`, providing the probability of a sample belonging to the positive class.\n",
    "- A loss function measures how well a machine learning model makes predictions. The loss function of Logistic Regression is log-loss.\n",
    "- A Classification Threshold is used to determine the probabilistic cutoff for where a data sample is classified as belonging to a positive or negative class. The standard cutoff for Logistic Regression is `0.5`, but the threshold can be higher or lower depending on the nature of the data and the situation.\n",
    "- Scikit-learn has a Logistic Regression implementation that allows you to fit a model to your data, find the feature coefficients, and make predictions on new data samples.\n",
    "- The coefficients determined by a Logistic Regression model can be used to interpret the relative importance of each feature in predicting the class of a data sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
